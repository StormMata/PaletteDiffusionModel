{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa7a909-b334-49a0-9a18-a0abb724cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import torch as py\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool, Manager\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3940b708-8482-444b-8723-262c8653274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# --------------------------------------\n",
    "ds1   = xr.open_dataset('/scratch/smata/data/ProcessedData_a0/second_pull/full_lidar.nc')\n",
    "ds24  = xr.open_dataset('/scratch/smata/data/ProcessedData_a0/second_pull/full_4m_sonic.nc')\n",
    "ds225 = xr.open_dataset('/scratch/smata/data/ProcessedData_a0/second_pull/full_25m_sonic.nc')\n",
    "ds260 = xr.open_dataset('/scratch/smata/data/ProcessedData_a0/second_pull/full_60m_sonic.nc')\n",
    "ds3   = xr.open_dataset('/scratch/smata/data/ProcessedData_a0/second_pull/full_stability.nc')\n",
    "# ds4   = xr.open_dataset('/scratch/smata/data/ProcessedData_a0/second_pull/full_sonde.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fda0378-7302-4792-945b-eebdd87242d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets on height dimension\n",
    "# --------------------------------------\n",
    "ds_combined = xr.concat([ds1, ds24, ds225, ds260], dim = 'height')\n",
    "ds_combined = ds_combined.sortby('height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4309dda5-9c8f-41eb-bde8-8c46b75fcd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This deletes outlier heights from datasets\n",
    "# --------------------------------------\n",
    "del_heights = xr.open_dataset('/scratch/smata/data/ProcessedData_a0/second_pull/sgpdlprofwind4newsC1.c1.20210407.000054.nc')['height']\n",
    "ds_combined = ds_combined.where(~ds_combined.height.isin(del_heights), drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0108928c-7947-4437-8917-540032898db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers velocities\n",
    "# --------------------------------------\n",
    "for i in range(ds_combined.height.shape[0]):\n",
    "    q90, q10 = np.nanpercentile(ds_combined['u'][i], [99.5, 0.5])\n",
    "    ds_combined['u'][i] = ds_combined['u'][i].where((ds_combined['u'][i] >= q10) & (ds_combined['u'][i] <= q90), np.nan)\n",
    "    q90, q10 = np.nanpercentile(ds_combined['v'][i], [99.5, 0.5])\n",
    "    ds_combined['v'][i] = ds_combined['v'][i].where((ds_combined['v'][i] >= q10) & (ds_combined['v'][i] <= q90), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74bb1245-b770-4deb-a065-bacd4eb8db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate to regular grid\n",
    "# --------------------------------------\n",
    "ds_combined = ds_combined.interp(height = np.arange(10, 2010, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b79200c-9d47-4a64-bb0a-6f16238f7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute common time axis and cyclical time variables\n",
    "# --------------------------------------\n",
    "\n",
    "ref_date      = np.datetime64('2015-07-23T00:00:00.000000')\n",
    "\n",
    "new_ref_times = np.empty(ds_combined.time.shape, dtype = 'datetime64[ns]')\n",
    "hr_per_day    = np.zeros(new_ref_times.shape)\n",
    "day_per_yr    = np.zeros(new_ref_times.shape)\n",
    "times         = ds_combined.time.values\n",
    "\n",
    "for i in range(ds_combined.time.shape[0]):\n",
    "    new_ref_times[i] = np.datetime64((pd.Timestamp(ref_date) + pd.Timedelta(times[i], 'D')).round('min'))\n",
    "    hr_per_day[i]    = pd.Timestamp(new_ref_times[i]).hour + pd.Timestamp(new_ref_times[i]).minute/60\n",
    "    day_per_yr[i]    = new_ref_times[i].astype('datetime64[D]').astype(datetime).timetuple().tm_yday\n",
    "\n",
    "ds_combined['hr_per_day_c'] = (('time'), np.cos(hr_per_day))\n",
    "ds_combined['hr_per_day_s'] = (('time'), np.sin(hr_per_day))\n",
    "ds_combined['day_per_yr_c'] = (('time'), np.cos(day_per_yr))\n",
    "ds_combined['day_per_yr_s'] = (('time'), np.sin(day_per_yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate copy/paste list of data bounds\n",
    "# --------------------------------------\n",
    "print(str(ds_combined['u'].min(skipna=True).values) + ',')\n",
    "print(str(ds_combined['u'].max(skipna=True).values) + ',')\n",
    "print(str(ds_combined['v'].min(skipna=True).values) + ',')\n",
    "print(str(ds_combined['v'].max(skipna=True).values) + ',')\n",
    "print(str(ds_combined['hr_per_day_c'].min(skipna=True).values) + ',')\n",
    "print(str(ds_combined['hr_per_day_c'].max(skipna=True).values) + ',')\n",
    "print(str(ds_combined['hr_per_day_s'].min(skipna=True).values) + ',')\n",
    "print(str(ds_combined['hr_per_day_s'].max(skipna=True).values) + ',')\n",
    "print(str(ds_combined['day_per_yr_c'].min(skipna=True).values) + ',')\n",
    "print(str(ds_combined['day_per_yr_c'].max(skipna=True).values) + ',')\n",
    "print(str(ds_combined['day_per_yr_s'].min(skipna=True).values) + ',')\n",
    "print(str(ds_combined['day_per_yr_s'].max(skipna=True).values) + ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training, validation, and testing files in parallel\n",
    "# --------------------------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    avg_u    = ds_combined['u'].mean(skipna=True)\n",
    "    avg_v    = ds_combined['v'].mean(skipna=True)\n",
    "    avg_hpdc = ds_combined['hr_per_day_c'].mean(skipna=True)\n",
    "    avg_hpds = ds_combined['hr_per_day_s'].mean(skipna=True)\n",
    "    avg_dpyc = ds_combined['day_per_yr_c'].mean(skipna=True)\n",
    "    avg_dpys = ds_combined['day_per_yr_s'].mean(skipna=True)\n",
    "\n",
    "    shared_trv = []\n",
    "    shared_te  = []\n",
    "\n",
    "    day = np.arange(0, np.floor(np.max(ds_combined.time)), 1)\n",
    "    # day = np.arange(0, 20, 1)\n",
    "    te_index = np.random.choice(day, int(np.floor(0.2 * len(day))), replace=False)\n",
    "\n",
    "    def generate_file(day_value, te_index, shared_te, shared_trv):\n",
    "\n",
    "        extract_u    = ds_combined.u.where((ds_combined.time >= day_value) & (ds_combined.time < (day_value + 1)), drop=True).values\n",
    "        extract_v    = ds_combined.v.where((ds_combined.time >= day_value) & (ds_combined.time < (day_value + 1)), drop=True).values\n",
    "\n",
    "        extract_hpdc = ds_combined.hr_per_day_c.where((ds_combined.time >= day_value) & (ds_combined.time < (day_value + 1)), drop=True).values\n",
    "        extract_hpds = ds_combined.hr_per_day_s.where((ds_combined.time >= day_value) & (ds_combined.time < (day_value + 1)), drop=True).values\n",
    "\n",
    "        extract_dpyc = ds_combined.day_per_yr_c.where((ds_combined.time >= day_value) & (ds_combined.time < (day_value + 1)), drop=True).values\n",
    "        extract_dpys = ds_combined.day_per_yr_s.where((ds_combined.time >= day_value) & (ds_combined.time < (day_value + 1)), drop=True).values\n",
    "\n",
    "        extract_u[np.isnan(extract_u)]       = avg_u\n",
    "        extract_v[np.isnan(extract_v)]       = avg_v\n",
    "        extract_hpdc[np.isnan(extract_hpdc)] = avg_hpdc\n",
    "        extract_hpds[np.isnan(extract_hpds)] = avg_hpds\n",
    "        extract_dpyc[np.isnan(extract_dpyc)] = avg_dpyc\n",
    "        extract_dpys[np.isnan(extract_dpys)] = avg_dpys\n",
    "\n",
    "        array_u      = extract_u.T\n",
    "        array_v      = extract_v.T\n",
    "\n",
    "        array_hpdc   = (np.ones((extract_u.shape)) * extract_hpdc).T\n",
    "        array_hpds   = (np.ones((extract_u.shape)) * extract_hpds).T\n",
    "        array_dpyc   = (np.ones((extract_u.shape)) * extract_dpyc).T\n",
    "        array_dpys   = (np.ones((extract_u.shape)) * extract_dpys).T\n",
    "\n",
    "        tensor = py.stack([py.Tensor(array_u), py.Tensor(array_v), py.Tensor(array_hpdc), py.Tensor(array_hpds), py.Tensor(array_dpyc), py.Tensor(array_dpys)])\n",
    "\n",
    "        if tensor.shape[1] == 96:\n",
    "\n",
    "            path = '/scratch/smata/data/ProcessedData_a0/train_val_test/day_%04d.pt' % day_value\n",
    "\n",
    "            if day_value in te_index:\n",
    "                shared_te.append(path + '\\n')\n",
    "            else:\n",
    "                shared_trv.append(path + '\\n')\n",
    "\n",
    "            py.save({'x': tensor, 'y': tensor}, path)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    start_time = time.time()\n",
    "\n",
    "    manager    = Manager()\n",
    "    shared_trv = manager.list()\n",
    "    shared_te  = manager.list()\n",
    "\n",
    "    # Parallel processing\n",
    "    with Pool(48) as pool:\n",
    "        pool.starmap(generate_file, [(d, te_index, shared_te, shared_trv) for d in day])\n",
    "\n",
    "    sorted_trv = sorted(shared_trv)\n",
    "    sorted_te  = sorted(shared_te)\n",
    "\n",
    "    with open('./train_and_val.flist', 'a') as file_trv, open('./test.flist', 'a') as file_te:\n",
    "        file_trv.writelines(sorted_trv)\n",
    "        file_te.writelines(sorted_te)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662b7c5",
   "metadata": {},
   "source": [
    "### Various plotting and QC scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f028e0b-a439-433f-b5bc-20683339403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot time-varying profiles\n",
    "# # --------------------------------------\n",
    "\n",
    "# fig,ax = plt.subplots(figsize = (15, 4))\n",
    "\n",
    "# cntf = ax.pcolormesh(ds_combined['time'], ds_combined['height'], ds_combined['u'], vmin = -8, vmax = 15)\n",
    "# ax.set_ylabel(\"$z$ [m]\", color = 'w')\n",
    "# ax.set_ylim([0, 2000])\n",
    "\n",
    "# cb_ax = fig.add_axes([0.905, 0.1125, 0.015, 0.768])\n",
    "# cbar = fig.colorbar(cntf, orientation='vertical', cax = cb_ax)\n",
    "# cbar.set_label('$u$ component [m/s]', rotation = 270, labelpad = 15, color = 'k')\n",
    "# # cbar.outline.set_color('white')\n",
    "# # cbar.ax.yaxis.set_tick_params(color = 'w')\n",
    "# # plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color = 'w')\n",
    "\n",
    "# # for spine in ax.spines.values():\n",
    "#     # spine.set_edgecolor('w')\n",
    "\n",
    "# ax.set_xticks((np.array([360, 725, 1090, 1440, 1800, 2160, 2520, 2880, 3240]) - 204), [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"])\n",
    "# # ax.set_title(f'Profiles: {common_time.shape[0]:,}', color = 'w')\n",
    "\n",
    "# # ax.tick_params(axis = 'x', colors = 'w')\n",
    "# # ax.tick_params(axis = 'y', colors = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664194b7-a4f9-4a1d-a1d3-4e830a13ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot time-varying profiles\n",
    "# # --------------------------------------\n",
    "\n",
    "# fig,ax = plt.subplots(figsize = (15, 4))\n",
    "\n",
    "# cntf = ax.pcolormesh(ds_combined['time'], ds_combined['height'], ds_combined['v'], vmin = -8, vmax = 20)\n",
    "# ax.set_ylabel(\"$z$ [m]\", color = 'w')\n",
    "# ax.set_ylim([0, 2000])\n",
    "\n",
    "# cb_ax = fig.add_axes([0.905, 0.1125, 0.015, 0.768])\n",
    "# cbar = fig.colorbar(cntf, orientation='vertical', cax = cb_ax)\n",
    "# cbar.set_label('$v$ component [m/s]', rotation = 270, labelpad = 15, color = 'k')\n",
    "# # cbar.outline.set_color('white')\n",
    "# # cbar.ax.yaxis.set_tick_params(color = 'w')\n",
    "# # plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color = 'w')\n",
    "\n",
    "# # for spine in ax.spines.values():\n",
    "#     # spine.set_edgecolor('w')\n",
    "\n",
    "# ax.set_xticks((np.array([360, 725, 1090, 1440, 1800, 2160, 2520, 2880, 3240]) - 204), [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"])\n",
    "# # ax.set_title(f'Profiles: {common_time.shape[0]:,}', color = 'w')\n",
    "\n",
    "# # ax.tick_params(axis = 'x', colors = 'w')\n",
    "# # ax.tick_params(axis = 'y', colors = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccf77e0d-6b03-4969-b92e-8da922a1bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot time-varying profiles\n",
    "# # --------------------------------------\n",
    "\n",
    "# fig,ax = plt.subplots(figsize = (15, 6))\n",
    "\n",
    "# cntf = ax.pcolormesh(ds_combined['time'], ds_combined['height'], (ds_combined['v']**2 + ds_combined['u']**2)**(0.5), vmin = 0, vmax = 20)\n",
    "# ax.set_ylabel(\"$z$ [m]\", color = 'k')\n",
    "# ax.set_ylim([0, 2000])\n",
    "\n",
    "# cb_ax = fig.add_axes([0.905, 0.1125, 0.015, 0.768])\n",
    "# cbar = fig.colorbar(cntf, orientation='vertical', cax = cb_ax)\n",
    "# cbar.set_label('Wind speed [m/s]', rotation = 270, labelpad = 15, color = 'k')\n",
    "# # cbar.outline.set_color('white')\n",
    "# # cbar.ax.yaxis.set_tick_params(color = 'w')\n",
    "# # plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color = 'w')\n",
    "\n",
    "# # for spine in ax.spines.values():\n",
    "#     # spine.set_edgecolor('w')\n",
    "\n",
    "# ax.set_xticks((np.array([360, 725, 1090, 1440, 1800, 2160, 2520, 2880, 3240]) - 204), [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"])\n",
    "# # ax.set_title(f'Profiles: {common_time.shape[0]:,}', color = 'w')\n",
    "\n",
    "# # ax.tick_params(axis = 'x', colors = 'w')\n",
    "# # ax.tick_params(axis = 'y', colors = 'w')\n",
    "\n",
    "# plt.savefig('./large_data.png', dpi = 700, bbox_inches = 'tight', transparent = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23d849c3-e065-4706-b98b-aeaaee2c1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confirm time axes are correct\n",
    "# # --------------------------------------\n",
    "\n",
    "# fig,ax = plt.subplots(figsize = (7, 7))\n",
    "\n",
    "# # ax[0].plot(times, hr_per_day)\n",
    "# ax.hist(hr_per_day, bins = 46, edgecolor = 'black')\n",
    "# # ax.set_xlim([0, 23])\n",
    "# # x[0].plot(range(len(times)), hr_per_day)\n",
    "# ax.set_xlabel('Fractional hour of day')\n",
    "# ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870c78a0-f848-4330-8ba3-350719d429fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot rectified wind profiles\n",
    "# # --------------------------------------\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "\n",
    "# fig,ax = plt.subplots(3, 1, figsize = (15, 8), sharex = True)\n",
    "\n",
    "# cntf = ax[0].pcolormesh(ds_combined['time'], ds_combined['height'], ds_combined['u'], vmin = -15, vmax = 15,  cmap = 'twilight_shifted')\n",
    "# cbar = plt.colorbar(cntf)\n",
    "# cbar.set_label('$u$ component [m/s]', rotation = 270, labelpad = 20)\n",
    "# ax[0].set_ylabel(\"$z$ [m]\")\n",
    "# ax[0].set_ylim([0, 2000])\n",
    "# ax[0].get_xaxis().set_ticks([])\n",
    "\n",
    "# cntf = ax[1].pcolormesh(ds_combined['time'], ds_combined['height'], ds_combined['v'], vmin = -15, vmax = 15,  cmap = 'twilight_shifted')\n",
    "# cbar = plt.colorbar(cntf)\n",
    "# cbar.set_label('$v$ component [m/s]', rotation = 270, labelpad = 20)\n",
    "# ax[1].set_ylabel(\"$z$ [m]\")\n",
    "# ax[1].set_ylim([0, 2000])\n",
    "# ax[1].get_xaxis().set_ticks([])\n",
    "\n",
    "# cntf = ax[2].pcolormesh(ds_combined['time'], ds_combined['height'], (ds_combined['u']**2 + ds_combined['v']**2)**(1/2), vmin = 0, vmax = 20, cmap = 'viridis')\n",
    "# cbar = plt.colorbar(cntf)\n",
    "# cbar.set_label('Wind speed [m/s]', rotation = 270, labelpad = 20)\n",
    "# # ax[2].set_xlabel(\"Profile index [-]\")\n",
    "# ax[2].set_ylabel(\"$z$ [m]\")\n",
    "# ax[2].set_ylim([0, 2000])\n",
    "# ax[2].set_xticks((np.array([360, 725, 1090, 1440, 1800, 2160, 2520, 2880, 3240]) - 204), [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"])\n",
    "\n",
    "# plt.savefig('./large_data_uv.png', dpi = 750, bbox_inches = 'tight', transparent = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e7e1a0-1cbb-4477-8e28-e7ac86cc41ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# day = 5\n",
    "\n",
    "# extract_u = ds_combined.u.where((ds_combined.time >= day) & (ds_combined.time < (day + 1)), drop=True)\n",
    "# extract_v = ds_combined.v.where((ds_combined.time >= day) & (ds_combined.time < (day + 1)), drop=True)\n",
    "\n",
    "# extract_t = ds_combined.time.where((ds_combined.time >= day) & (ds_combined.time < (day + 1)), drop=True)\n",
    "\n",
    "# fig,ax = plt.subplots(figsize = (6, 8))\n",
    "\n",
    "# cntf = ax.pcolormesh(extract_t, ds_combined['height'], (extract_u**(2) + extract_v)**(0.5), vmin = 0, vmax = 15)\n",
    "# ax.set_ylabel(\"$z$ [m]\")\n",
    "# ax.set_xlabel(\"Day Index [-]\")\n",
    "# ax.set_ylim([0, 2000])\n",
    "\n",
    "# cb_ax = fig.add_axes([0.91, 0.11, 0.025, 0.77])\n",
    "# cbar = fig.colorbar(cntf, orientation='vertical', cax = cb_ax)\n",
    "# cbar.set_label('Wind Speed [m/s]', rotation = 270, labelpad = 12, color = 'k')\n",
    "\n",
    "# plt.savefig('./large_data.png', dpi = 700, bbox_inches = 'tight', transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate training, validation, and testing files in serial\n",
    "# # --------------------------------------\n",
    "\n",
    "# avg_u    = ds_combined['u'].mean(skipna=True)\n",
    "# avg_v    = ds_combined['v'].mean(skipna=True)\n",
    "# avg_hpdc = ds_combined['hr_per_day_c'].mean(skipna=True)\n",
    "# avg_hpds = ds_combined['hr_per_day_s'].mean(skipna=True)\n",
    "# avg_dpyc = ds_combined['day_per_yr_c'].mean(skipna=True)\n",
    "# avg_dpys = ds_combined['day_per_yr_s'].mean(skipna=True)\n",
    "\n",
    "# file_trv = open('./train_and_val.flist', 'a')\n",
    "# file_te  = open('./test.flist', 'a')\n",
    "\n",
    "# day = np.arange(0, np.floor(np.max(ds_combined.time)), 1)\n",
    "# # day = np.arange(0, 20, 1)\n",
    "# te_index = np.random.choice(day, int(np.floor(0.2 * len(day))), replace=False)\n",
    "\n",
    "# for i in range(len(day)):\n",
    "\n",
    "#     extract_u    = ds_combined.u.where((ds_combined.time >= day[i]) & (ds_combined.time < (day[i] + 1)), drop=True).values\n",
    "#     extract_v    = ds_combined.v.where((ds_combined.time >= day[i]) & (ds_combined.time < (day[i] + 1)), drop=True).values\n",
    "\n",
    "#     extract_hpdc = ds_combined.hr_per_day_c.where((ds_combined.time >= day[i]) & (ds_combined.time < (day[i] + 1)), drop=True).values\n",
    "#     extract_hpds = ds_combined.hr_per_day_s.where((ds_combined.time >= day[i]) & (ds_combined.time < (day[i] + 1)), drop=True).values\n",
    "\n",
    "#     extract_dpyc = ds_combined.day_per_yr_c.where((ds_combined.time >= day[i]) & (ds_combined.time < (day[i] + 1)), drop=True).values\n",
    "#     extract_dpys = ds_combined.day_per_yr_s.where((ds_combined.time >= day[i]) & (ds_combined.time < (day[i] + 1)), drop=True).values\n",
    "\n",
    "#     extract_u[np.isnan(extract_u)]       = avg_u\n",
    "#     extract_v[np.isnan(extract_v)]       = avg_v\n",
    "#     extract_hpdc[np.isnan(extract_hpdc)] = avg_hpdc\n",
    "#     extract_hpds[np.isnan(extract_hpds)] = avg_hpds\n",
    "#     extract_dpyc[np.isnan(extract_dpyc)] = avg_dpyc\n",
    "#     extract_dpys[np.isnan(extract_dpys)] = avg_dpys\n",
    "\n",
    "#     array_u      = extract_u.T\n",
    "#     array_v      = extract_v.T\n",
    "\n",
    "#     array_hpdc   = (np.ones((extract_u.shape)) * extract_hpdc).T\n",
    "#     array_hpds   = (np.ones((extract_u.shape)) * extract_hpds).T\n",
    "#     array_dpyc   = (np.ones((extract_u.shape)) * extract_dpyc).T\n",
    "#     array_dpys   = (np.ones((extract_u.shape)) * extract_dpys).T\n",
    "\n",
    "#     tensor = py.stack([py.Tensor(array_u), py.Tensor(array_v), py.Tensor(array_hpdc), py.Tensor(array_hpds), py.Tensor(array_dpyc), py.Tensor(array_dpys)])\n",
    "\n",
    "#     if i in te_index:\n",
    "#         path = '/scratch/smata/data/ProcessedData_a0/test/day_%04d.pt' % i\n",
    "#         file_te.write(path + '\\n')\n",
    "#     else:\n",
    "#         path = '/scratch/smata/data/ProcessedData_a0/train_and_validation/day_%04d.pt' % i\n",
    "#         file_trv.write(path + '\\n')\n",
    "\n",
    "#     py.save({'x': tensor, 'y': tensor}, '/scratch/smata/data/ProcessedData_a0/train_val_test/day_%04d.pt' % i)\n",
    "\n",
    "# file_trv.close()\n",
    "# file_te.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8fad715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate movie of snapshots\n",
    "# --------------------------------------\n",
    "day = np.arange(1, 11, 1/96)\n",
    "\n",
    "for i in range(len(day)):\n",
    "\n",
    "    extract_u = ds_combined.u.where((ds_combined.time >= day[i]) & (ds_combined.time < (day[i] + 1)), drop=True)\n",
    "    extract_v = ds_combined.v.where((ds_combined.time >= day[i]) & (ds_combined.time < (day[i] + 1)), drop=True)\n",
    "\n",
    "    extract_t = ds_combined.time.where((ds_combined.time >= day[i]) & (ds_combined.time < (day[i] + 1)), drop=True)\n",
    "\n",
    "    plt.style.use('dark_background')\n",
    "\n",
    "    fig,ax = plt.subplots(figsize = (6, 8))\n",
    "\n",
    "    cntf = ax.pcolormesh(extract_t, ds_combined['height'], (extract_u**(2) + extract_v)**(0.5), vmin = 0, vmax = 15)\n",
    "    ax.set_ylabel(\"$z$ [m]\")\n",
    "    ax.set_xlabel(\"Day Index [-]\")\n",
    "    ax.set_ylim([0, 2000])\n",
    "\n",
    "    cb_ax = fig.add_axes([0.91, 0.11, 0.025, 0.77])\n",
    "    cbar = fig.colorbar(cntf, orientation='vertical', cax = cb_ax)\n",
    "    cbar.set_label('Wind Speed [m/s]', rotation = 270, labelpad = 12)\n",
    "\n",
    "    plt.savefig('./snapshots/snap_%04d.png' % i, dpi = 420, bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c563dcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3288815/2119723161.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mmm = py.load('./data/ProcessedData_a0/train_and_validation/day_0000.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.5263, -2.6269, -2.6848,  ...,  1.7598,  1.7598,  1.7598],\n",
       "         [-2.7631, -3.1496, -3.3630,  ...,  1.7598,  1.7598,  1.7598],\n",
       "         [-2.9373, -3.3360, -3.5931,  ...,  1.7598,  1.7598,  1.7598],\n",
       "         ...,\n",
       "         [-1.5537, -1.5712, -1.5499,  ...,  1.7598,  1.7598,  1.7598],\n",
       "         [-2.4460, -2.5391, -2.5325,  ...,  1.7598,  1.7598,  1.7598],\n",
       "         [-1.7238, -1.7678, -1.7713,  ...,  1.7598,  1.7598,  1.7598]],\n",
       "\n",
       "        [[ 2.9857,  3.6750,  4.1971,  ...,  3.7802,  3.7802,  3.7802],\n",
       "         [ 2.8252,  3.7040,  4.3167,  ...,  3.7802,  3.7802,  3.7802],\n",
       "         [ 2.4222,  3.1553,  3.7104,  ...,  3.7802,  3.7802,  3.7802],\n",
       "         ...,\n",
       "         [ 4.1611,  4.8368,  5.3223,  ...,  3.7802,  3.7802,  3.7802],\n",
       "         [ 3.8206,  4.6825,  5.2942,  ...,  3.7802,  3.7802,  3.7802],\n",
       "         [ 4.1302,  4.7847,  5.2892,  ...,  3.7802,  3.7802,  3.7802]],\n",
       "\n",
       "        [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         [ 0.9689,  0.9689,  0.9689,  ...,  0.9689,  0.9689,  0.9689],\n",
       "         [ 0.8776,  0.8776,  0.8776,  ...,  0.8776,  0.8776,  0.8776],\n",
       "         ...,\n",
       "         [-0.3069, -0.3069, -0.3069,  ..., -0.3069, -0.3069, -0.3069],\n",
       "         [-0.0619, -0.0619, -0.0619,  ..., -0.0619, -0.0619, -0.0619],\n",
       "         [ 0.1869,  0.1869,  0.1869,  ...,  0.1869,  0.1869,  0.1869]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.2474,  0.2474,  0.2474,  ...,  0.2474,  0.2474,  0.2474],\n",
       "         [ 0.4794,  0.4794,  0.4794,  ...,  0.4794,  0.4794,  0.4794],\n",
       "         ...,\n",
       "         [-0.9517, -0.9517, -0.9517,  ..., -0.9517, -0.9517, -0.9517],\n",
       "         [-0.9981, -0.9981, -0.9981,  ..., -0.9981, -0.9981, -0.9981],\n",
       "         [-0.9824, -0.9824, -0.9824,  ..., -0.9824, -0.9824, -0.9824]],\n",
       "\n",
       "        [[-0.6992, -0.6992, -0.6992,  ..., -0.6992, -0.6992, -0.6992],\n",
       "         [-0.6992, -0.6992, -0.6992,  ..., -0.6992, -0.6992, -0.6992],\n",
       "         [-0.6992, -0.6992, -0.6992,  ..., -0.6992, -0.6992, -0.6992],\n",
       "         ...,\n",
       "         [-0.6992, -0.6992, -0.6992,  ..., -0.6992, -0.6992, -0.6992],\n",
       "         [-0.6992, -0.6992, -0.6992,  ..., -0.6992, -0.6992, -0.6992],\n",
       "         [-0.6992, -0.6992, -0.6992,  ..., -0.6992, -0.6992, -0.6992]],\n",
       "\n",
       "        [[-0.7149, -0.7149, -0.7149,  ..., -0.7149, -0.7149, -0.7149],\n",
       "         [-0.7149, -0.7149, -0.7149,  ..., -0.7149, -0.7149, -0.7149],\n",
       "         [-0.7149, -0.7149, -0.7149,  ..., -0.7149, -0.7149, -0.7149],\n",
       "         ...,\n",
       "         [-0.7149, -0.7149, -0.7149,  ..., -0.7149, -0.7149, -0.7149],\n",
       "         [-0.7149, -0.7149, -0.7149,  ..., -0.7149, -0.7149, -0.7149],\n",
       "         [-0.7149, -0.7149, -0.7149,  ..., -0.7149, -0.7149, -0.7149]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect sample files\n",
    "# --------------------------------------\n",
    "mmm = py.load('./data/ProcessedData_a0/train_and_validation/day_0000.pt')\n",
    "\n",
    "type(mmm)\n",
    "\n",
    "mmm.keys()\n",
    "\n",
    "mmm['x'].shape\n",
    "\n",
    "mmm['x']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
